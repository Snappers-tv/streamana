<html>
<head>
<script type="text/javascript" src="https://unpkg.com/glsl-canvas-js/dist/umd/glsl-canvas.min.js"></script>
<script type="text/javascript">
// Use setInterval instead of requestAnimation frame so video continues
// even when tab is hidden
class GlCanvas extends glsl.Canvas {
  onLoop() {
    this.checkRender();
    this.siId = setInterval(() => this.checkRender(), 33);
  }

  destroy() {
    clearInterval(this.siId);
  }
}

let stream_url_el, start_el, stop_el, pause_el, resume_el, monitor_el;

function init() {
  stream_url_el = document.getElementById('stream_url');
  start_el = document.getElementById('start');
  stop_el = document.getElementById('stop');
  pause_el = document.getElementById('pause');
  resume_el = document.getElementById('resume');
  monitor_el = document.getElementById('monitor');
}

let camera_stream, gl_canvas, canvas_stream, recorder, ffmpeg_hls;

async function start() {
  const stream_url = stream_url_el.value.trim();
  if (!stream_url) {
    return;
  }

  start_el.disabled = true;

  // capture video from webcam
  camera_stream = await navigator.mediaDevices.getUserMedia({
    audio: true,
    video: {
      width: 4096,
      height: 2160,
      frameRate: {
        ideal: 30,
        max: 30
      }
    }
  });

  // create video element which will be used for grabbing the frames to
  // write to a canvas so we can apply webgl shaders
  // also used to get the native video dimensions
  const video = document.createElement("video");
  video.muted = true;

  // create a canvas for doing webgl
  const canvas = document.createElement('canvas');

  // use glsl-canvas to make managing webgl stuff easier
  // because it's not visible, client dimensions are zero so we
  // need to substitute actual dimensions instead
  gl_canvas = new GlCanvas(new Proxy(canvas, {
    get: function (target, name, receiver) {
      if (name === 'getBoundingClientRect') {
        return () => new DOMRect(0, 0, target.width, target.height);
      }
      if (name === 'clientWidth') {
        return target.width;
      }
      if (name === 'clientHeight') {
        return target.height;
      }
      const r = target[name];
      return typeof r === 'function' ? r.bind(target) : r;
    },
    set: function (target, name, value) {
      target[name] = value;
      return true;
    }
  }));

  // sample greyscale fragment shader
  const fragmentShader = `
precision highp float;

uniform sampler2D u_texture;
uniform vec2 u_resolution;

void main() {
  vec2 st = gl_FragCoord.xy / u_resolution.xy;
  vec3 color = texture2D(u_texture, st).rgb;
  float grey = dot(color, vec3(0.299, 0.587, 0.114));
  gl_FragColor = vec4(vec3(grey), 1.0);
}`;
  gl_canvas.load(fragmentShader);

  // tell canvas to use frames from video
  gl_canvas.setTexture('u_texture', video);

  // wait for video to load (must come after gl_canvas.setTexture since it
  // registers a loadeddata handler which then registers a play handler)
  video.addEventListener('loadeddata', function () {
    // make canvas same size as native video dimensions so every pixel is seen
    canvas.width = this.videoWidth;
    canvas.height = this.videoHeight;

    // start the camera video
    this.play();

    // capture video from the canvas
    canvas_stream = canvas.captureStream(30);
    canvas_stream.addTrack(camera_stream.getAudioTracks()[0]);

    // set up video recording from the canvas; note we don't start
    // recording until ffmpeg has started (below)
    recorder = new MediaRecorder(canvas_stream, {
      mimeType: "video/webm;codecs=H264",
      audioBitsPerSecond:  128 * 1000,
      videoBitsPerSecond: 2500 * 1000
    });

    // push encoded data into the ffmpeg worker that we start below
    recorder.ondataavailable = async function (event) {
      const data = await event.data.arrayBuffer();
      ffmpeg_hls.postMessage({
        type: 'video-data',
        data: data
      }, [data]);
    };

    // start ffmpeg in a worker
    ffmpeg_hls = new Worker('ffmpeg.js/ffmpeg-worker-hls.js');
    ffmpeg_hls.onmessage = function (e) {
      const msg = e.data;
      switch (msg.type) {
        case 'ready':
          this.postMessage({
            type: 'run',
            arguments: [
              '-i', '-', // our worker will simulate stdin
              '-f', 'hls', // use hls encoder
              '-c:v', 'copy', // pass through the video data (h264, no decoding or encoding)
              '-c:a', 'aac',  // re-encode audio as AAC-LC
              '-b:a', '128k', // set audio bitrate
              '-hls_time', '2', // 2 second HLS chunks
              '-hls_segment_type', 'mpegts', // MPEG2-TS muxer
              '-hls_list_size', '2', // two chunks in the list at a time
              '/outbound/output.m3u8' // path to media playlist file in virtual FS,
                                      // must be under /outbound
            ]
          });
          break;
        case 'run':
          console.log("RUNNING");
          break;
        case 'stdout':
          console.log(msg.data);
          break;
        case 'stderr':
          console.error(msg.data);
          break;
        case 'exit':
          console.log("EXITED", msg.data);
          if (recorder.state !== 'inactive') {
            recorder.stop();
          }
          this.terminate();
          monitor_el.srcObject = null;
          start_el.disabled = false;
          break;
        case 'done':
          // due to async stdin read, we get this message immediately
          //console.log("DONE", msg.data);
          break;
        case 'error':
          console.error("ERROR", msg.data);
          break;
        case 'abort':
          console.error("ABORT", msg.data);
          break;
        case 'start-video':
          this.postMessage({
            type: 'base-url',
            data: stream_url
          });
          // start recording; produce data every second, we'll be chunking it anyway
          recorder.start(1000);
          break;
      }
    };

    // display the video locally so we can see what's going on
    // note the video seems to set its height automatically to keep the
    // correct aspect ratio
    monitor_el.srcObject = canvas_stream;
    monitor_el.play();

    stop_el.disabled = false;
    pause_el.disabled = false;
    resume_el.disabled = true;
  });

  // pass the stream from the camera to the video so it can render the frames
  video.srcObject = camera_stream;
}

function stop() {
  stop_el.disabled = true;
  pause_el.disabled = true;
  resume_el.disabled = true;

  for (let track of camera_stream.getTracks()) {
    track.stop();
  }

  gl_canvas.destroy();

  for (let track of canvas_stream.getTracks()) {
    track.stop();
  }
  
  ffmpeg_hls.postMessage({
    type: 'video-ended'
  });
}

function pause() {
  for (let track of camera_stream.getTracks()) {
    track.enabled = false;
  }
  pause_el.disabled = true;
  resume_el.disabled = false;
}

function resume() {
  for (let track of camera_stream.getTracks()) {
    track.enabled = true;
  }
  pause_el.disabled = false;
  resume_el.disabled = true;
}
</script>
</head>
<body onload="init()">
<p>
  <input id=stream_url type="text" placeholder="Youtube Stream URL">
  <input id=start type="button" value="Start" onclick="start()">
  <input id=stop type="button" value="Stop" disabled onclick="stop()">
  <input id=pause type="button" value="Pause" disabled onclick="pause()">
  <input id=resume type="button" value="Resume" disabled onclick="resume()">
</p>
<video id="monitor" style="width:100%" muted="true"></video>
</body>
</html>
